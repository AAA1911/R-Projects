Here i want to explain what i did.
1) I've chosen 2 Geroge R.R. Martin's books from Song of Ice and Fire series as my 100 html pages. 
2) I imported urls' in R, parsed them using function cleanFun from StackOverflow and saved them in 'texts' folder. (For that i used 'downloadttxt.R' and 'download pages 2.R' files)
3) Them i'm importing them into R as directory to create "VCorpus" "Corpus" object to get "a list of at least 20 terms (words/ phrases) characteristic for the topic." This list is 20 most frequently used words in 2 books combined.
4) So as a result of some operations that i made i get freq for words used. Vector of top 20 words is the following:
word = c('said', 'lord', 'ser', 'man', "back", "men", "see", "king", 'eyes', 'never', 
'told', 'know', 'father', 'old', 'black', 'thought', 'time', 'fire', 'face', 'brother')
5) I am turning this vector into a dataframe to use it in creation requency distribution per page.
6) Then i am switching to "creation page: topwords dataframe.R" file to count words for each page. I know that it can be done mych easier and my coed looks awful but by using that i achieved atleast some result which seems more less valid for me. I used tokenization for each page, count all words and turn them into single dataframes so i got 1 dataframe for 1 page. Then i joined everything with top 20 words from the whole 2 books dataframe which i created at step 5 pairwise to be sured i took only words i am interested in. Then i joined everything again to a common dataframe, made some manipulations like transposing the whole dataset, changing variables' names. As a result of this step i got "data frame with pages as units (rows) and selected terms as variables (columns)."
7) Then i get back to main file. I had to perform any kind of analysis, so i decided to do clusters of pages. To do that, firstly i had to recode all my columns from factors to numeric( i failed in recoding everything at the same time so i had to recode each column separately), and, secodnly, i had to handle with NA. As it was written in slides of one of our lectures, there are several ways to cope with NA. Because in this case NA means there were no such words on page, i changed al NAs' to 0.
8) And finally i did k-means cluster. R told me 3 clusters is enough. Although I understand that what i got is not very useful becase these are just pages grouped by words i think this can be ok for example if we change our top 20 words from entire book dataframe to a dataframe with character names which we are interseted in. In this case we could group pages by characters which appeared there.
